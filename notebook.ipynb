{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- langchain\n",
    "- langchain-community\n",
    "- chromadb\n",
    "- pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "DOCUMENTS_PATH = \"./pdfs\"\n",
    "CHROMA_PATH = \"./chroma\"\n",
    "PROMPT_TEMPLATE=\"\"\"\n",
    "ANSWER THE QUESTION BASED ONLY ON THE FOLLOWING CONTEXT:\n",
    "{context}\n",
    "\n",
    "---\n",
    "ANSWER THE FOLLOWING QUESTION BASED ONLY ON THE CONTEXT ABOVE: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type imports\n",
    "from typing import Iterable\n",
    "from chromadb import Embeddings\n",
    "\n",
    "# Imports\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentLoader():\n",
    "  \"\"\"Load all documents from the pdfs directory\"\"\"\n",
    "  return PyPDFDirectoryLoader(DOCUMENTS_PATH).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textSplitter(documents: Iterable[Document]):\n",
    "  \"\"\"Split the text of each document into characters\"\"\"\n",
    "  return RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 80\n",
    "  ).split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollamaEmbedder():\n",
    "  \"\"\"Embed the characters of each document\"\"\"\n",
    "  return OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollamaLLM():\n",
    "  \"\"\"Create an Ollama language model object\"\"\"\n",
    "  return Ollama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initChromaDB(embedder: Embeddings):\n",
    "  \"\"\"Create a Chroma vector store from the documents\"\"\"\n",
    "  return Chroma(persist_directory=CHROMA_PATH, embedding_function=embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkIdGen(chunks: list[Document]):\n",
    "  \"\"\"Create an id for a chunk based on document, page and position\"\"\"\n",
    "  currSource = \"\"\n",
    "  currPage = -1\n",
    "  currPosition = -1\n",
    "  \n",
    "  for chunk in chunks:\n",
    "    source = chunk.metadata.get(\"source\")\n",
    "    page = chunk.metadata.get(\"page\")\n",
    "    currPosition = currPosition + 1 if source == currSource and page == currPage else 0\n",
    "    currSource = source\n",
    "    currPage = page\n",
    "    chunk.metadata[\"id\"] = f\"{source}:{page}:{currPosition}\"\n",
    "\n",
    "def updateChromaDB(db: Chroma, chunks: list[Document]):\n",
    "  \"\"\"Update a Chroma vector store with new documents\"\"\"\n",
    "  dbItems = db.get(include=[])\n",
    "  existingIds = set(dbItems[\"ids\"])\n",
    "  \n",
    "  newChunks = [chunk for chunk in chunks if chunk.metadata.get(\"id\") not in existingIds]\n",
    "  chunkIdGen(newChunks)\n",
    "  \n",
    "  if len(newChunks) > 0:\n",
    "    print(f\"Adding {len(newChunks)} new chunks to the Chroma database\")\n",
    "    db.add_documents(newChunks, ids=[chunk.metadata.get(\"id\") for chunk in newChunks])\n",
    "    db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchDBForQueryContext(query: str, db: Chroma):\n",
    "  \"\"\"Query the Chroma vector store for contexts similar to the query\"\"\"\n",
    "  results = db.similarity_search_with_score(query, k=5)\n",
    "  return (\n",
    "    \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results]),\n",
    "    [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatPrompt(context: str, question: str):\n",
    "  \"\"\"Format a prompt for the user\"\"\"\n",
    "  return PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "#ChatPromptTemplate.from_template(PROMPT_TEMPLATE).format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "documents = documentLoader()\n",
    "\n",
    "# Split the text of each document into characters\n",
    "chunks = textSplitter(documents)\n",
    "\n",
    "# Load embedding function\n",
    "embedder = ollamaEmbedder()\n",
    "\n",
    "# Init and load the Chroma vector store\n",
    "db = initChromaDB(embedder)\n",
    "updateChromaDB(db, chunks)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example\n",
    "```py\n",
    "model = ollamaLLM()\n",
    "\n",
    "question = \"What is a flush?\"\n",
    "(context, sources) = searchDBForQueryContext(question, db)\n",
    "\n",
    "prompt = formatPrompt(context, question)\n",
    "response = model.invoke(prompt)\n",
    "\n",
    "print(prompt)\n",
    "print(response)\n",
    "print(sources)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ollamaLLM()\n",
    "userInput = \"\"\n",
    "\n",
    "print(\"You can make questions about UNO, Texas Hold'em, or TBOI Four Souls.\")\n",
    "print(\"Exit by typing 'exit'.\")\n",
    "while userInput != \"exit\":\n",
    "  userInput = input(\"> \")\n",
    "  context, sources = searchDBForQueryContext(userInput, db)\n",
    "  prompt = formatPrompt(context, userInput)\n",
    "  response = model.invoke(prompt)\n",
    "  print(response)\n",
    "  print(\"Sources:\" + str(sources))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
